@inproceedings{IEEE:conf/ICIIT/,
  abbr      = {},
  author    = {},
  title     = {Deep Learning-Based Automated Cashier System for Bakeries},
  booktitle = {},
  publisher = {},
  year      = {},
  code      = {},
  abstract  = {},
  dimensions= {true},
  selected  = {},
  preview   = {},
}

@inproceedings{IEEE:conf/ICIIT/,
  abbr      = {},
  author    = {},
  title     = {Innovative Multi-Modal Control for Surveillance Spider Robots:
Voice and Hand Gesture Integration},
  booktitle = {},
  publisher = {},
  year      = {},
  code      = {},
  abstract  = {},
  dimensions= {true},
  selected  = {},
  preview   = {},
}

@inproceedings{IEEE:conf/ICIIT/,
  abbr      = {},
  author    = {},
  title     = {Vietnamese Traffic Sign Recognition Using Deep Learning},
  booktitle = {},
  publisher = {},
  year      = {},
  code      = {},
  abstract  = {},
  dimensions= {true},
  selected  = {},
  preview   = {},
}

@inproceedings{Springer:conf/EAI-INISCOM/PerfTSR,
abbr      = {EAI INISCOM},
author    = {Nguyen, Huu Duc and Thanh, Nam Vo and Tran, Phuong-Nam and Nguyen, Cuong Tuan and Dang, Duc Ngoc Minh },
editor    = {},
title     = {Performance Comparison in Traffic Sign Recognition using Deep Learning},
booktitle = {},
year      = {2024},
publisher = {},
address   = {},
pages     = {},
abstract  = {In recent years, along with the increase in private cars, traffic signs have increased in quantity, demanding greater attentiveness from drivers. Many studies have been conducted on Traffic Sign Recognition (TSR) to enhance road safety and driver assistance systems by enabling vehicles to autonomously detect and interpret traffic signs, providing crucial information to drivers in real-time. This paper examines various traffic sign detection and classification models, which give practitioners and researchers valuable insights into selecting optimal solutions tailored to the needs of real-world applications. Notably, the investigation highlights YOLOv8 as a leading detection model, displaying exceptional results with an mAP of 99.4\%. YOLOv8 provides various model sizes allowing for adaptation to specific real-time processing scenarios. On the other hand, the LeNet model is a standout performer in the classification domain, consistently achieving a remarkable accuracy of 98.2\% while using only 0.4 million parameters. The LeNet architecture ensures accurate and rapid traffic sign classification, making it an appealing choice for applications where resource efficiency is critical.},
isbn      = {},
code      = {},
selected  = {},
dimensions= {true},
html      = {},
doi       = {},
}


@inproceedings{IEEE:conf/ICOIN/MERSA,
  abbr      = {ICOIN},
  author    = {Bao Le, Quan  and Tuan Trinh, Kiet and Pham, Nhat Truong and Nguyen, Dinh Hung Son and Tran, Phuong-Nam and Tuan Nguyen, Cuong  and Dang, Duc Ngoc Minh},
  title     = {MERSA: Multimodal Emotion Recognition with Self-Align Embedding},
  booktitle = {},
  publisher = {},
  year      = {2024},
  code      = {},
  abstract  = {Emotions are an integral part of human communication and interaction, significantly shaping our social connections, decision-making, and overall well-being. Understanding and analyzing emotions have become essential in various fields, including psychology, human-computer interaction, marketing, and healthcare.  The previous approach has indeed made significant strides in improving the accuracy of predicting emotions within speech. However, the current model's performance still falls short when it comes to real-life applications. This limitation arises due to several factors such as lack of context, ambiguity in speech and meaning, and other contributing elements. To reduce the ambiguity of emotions within speech, this paper seeks to leverage multiple data modalities, specifically textual and acoustic information. To analyze these modalities, we propose a novel approach called MERSA which utilizes the self-align method to extract context features from both textual and acoustic information. By leveraging this technique, the MERSA model can effectively create fusion feature vectors of the multiple inputs, facilitating a more accurate and holistic analysis of emotions within speech. Moreover, the MERSA model has incorporated a cross-attention module into its network architecture, which enables the MERSA model to capture and leverage the interdependencies between the textual and acoustic modalities.},
  dimensions= {true},
  selected  = {true},
  preview   = {icoin/mersa.png},
}

@inproceedings{IEEE:conf/ICTC/ComSER,
  abbr      = {ICTC},
  author    = {Tran, Phuong-Nam and Vu, Thuy-Duong Thi and Pham, Nhat Truong and Dang-Ngoc, Hanh and Dang, Duc Ngoc Minh},
  title     = {Comparative analysis of multi-loss functions for enhanced multi-modal speech emotion recognition},
  booktitle = {{ICTC} 2023: 14th International Conference on ICT Convergence, Jeju, Korea, Oct 11-13, 2023},
  publisher = {{IEEE}},
  year      = {2023},
  code      = {https://github.com/namphuongtran9196/3m-ser},
  abstract  = {In recent years, multi-modal analysis has gained significant prominence across domains such as audio/speech processing, natural language processing, and affective computing, with a particular focus on speech emotion recognition (SER). The integration of data from diverse sources, encompassing text, audio, and images, in conjunction with classifier algorithms has led to the realization of enhanced performance in SER tasks. Traditionally, the cross-entropy loss function has been employed for the classification problem. However, it is challenging to discriminate the feature representations among classes for multi-modal classification tasks. In this study, we focus on the impact of the loss functions on multi-modal SER rather than designing the model architecture. Mainly, we evaluate the performance of multi-modal SER with different loss functions, such as cross-entropy loss, center loss, contrastive-center loss, and their combinations. Based on extensive comparative analysis, it is proven that the combination of cross-entropy loss and contrastive-center loss achieves the best performance for multi-modal SER. This combination reaches the highest accuracy of 80.27\% and the highest balanced accuracy of 81.44\% on the IEMOCAP dataset.},
  dimensions= {true}
}

@inproceedings{IEEE:conf/ICTC/RBBA,
  abbr      = {ICTC},
  author    = {Hoang, Duc-Hieu and Dang, Duc Ngoc Minh and Dang-Ngoc, Hanh and Tran, Anh-Khoa and Tran, Phuong-Nam and Nguyen, Cuong Tuan},
  title     = {RBBA: ResNet - BERT - Bahdanau Attention for Image Caption Generator},
  booktitle = {{ICTC} 2023: 14th International Conference on ICT Convergence, Jeju, Korea, Oct 11-13, 2023},
  publisher = {{IEEE}},
  year      = {2023},
  abstract  = {In recent years, the topic of image caption generators has gained significant attention. Several successful projects have emerged in this field, showcasing notable advancements. Image caption generators automatically generate descriptive captions for images through the encoder and decoder mechanisms. The encoder leverages computer vision models, while the decoder utilizes natural language processing models. In this study, we aim to assess a comprehensive set of seven distinct methodologies, including six existing methods from prior research and one newly proposed. These methods are trained and evaluated with bilingual evaluation (BLEU) on the Flickr8K dataset. In our experiments, the proposed ResNet50 - BERT - Bahdanau Attention model outperforms the other models in terms of the BLEU-1 score of 0.532143 and BLEU-4 score of 0.126316.},
  dimensions= {true}
}

@inproceedings{IEEE:conf/ICTC/Vitexco,
  abbr      = {ICTC},
  author    = {Tran, Duong Thanh  and Nguyen, Doan Hieu Nguyen and Pham, Trung Thanh and Tran, Phuong-Nam and Vu, Thuy-Duong Thi and Dang, Duc Ngoc Minh},
  title     = {Vitexco: Exemplar-based Video Colorization using Vision Transformer},
  booktitle = {{ICTC} 2023: 14th International Conference on ICT Convergence, Jeju, Korea, Oct 11-13, 2023},
  publisher = {{IEEE}},
  year      = {2023},
  abstract  = {In the field of image and video colorization, the existing research employs a CNN to extract information from each video frame. However, due to the local nature of a kernel, it is challenging for CNN to capture the relationships between each pixel and others in an image, leading to inaccurate colorization. To solve this issue, we introduce an end-to-end network called Vitexco for colorizing videos. Vitexco utilizes the power of the Vision Transformer (ViT) to capture the relationships among all pixels in a frame with each other, providing a more effective method for colorizing video frames. We evaluate our approach on DAVIS datasets and demonstrate that it outperforms the state-of-the-art methods regarding color accuracy and visual quality. Our findings suggest that using a ViT can significantly enhance the performance of video colorization.},
  dimensions= {true}
}